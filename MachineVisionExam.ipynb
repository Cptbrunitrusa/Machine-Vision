{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNtsqmrR-koi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Installs necessary to utilize the dataset and pre-training\n",
        "!pip install kaggle\n",
        "!pip install timm==0.6.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLCpHcIUBFEi"
      },
      "outputs": [],
      "source": [
        "# Imports all necessary functions\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models, datasets\n",
        "from torchvision.models import ResNet50_Weights, MobileNet_V3_Large_Weights\n",
        "from torchvision.datasets import ImageFolder\n",
        "from google.colab import drive\n",
        "\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxv_34nRBKOp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Used to access google drive, for storage and dataset extraction.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copies kaggle.json to Colab and configure Kaggle API\n",
        "!cp '/content/drive/Mydrive/kaggle.json' '/content/'\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d ankit1743/skyview-an-aerial-landscape-dataset\n",
        "!unzip skyview-an-aerial-landscape-dataset.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHogQLh27hdd"
      },
      "outputs": [],
      "source": [
        "# Dataset path (in google drive)\n",
        "dataset_path = '/content/Aerial_Landscapes'\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# C2: Cutout augmentation\n",
        "class Cutout:\n",
        "    def __init__(self, num_holes, hole_size):\n",
        "        self.num_holes = num_holes\n",
        "        self.hole_size = hole_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = torch.ones((h, w), dtype=torch.float32)\n",
        "\n",
        "        for _ in range(self.num_holes):\n",
        "            y = random.randint(0, h)\n",
        "            x = random.randint(0, w)\n",
        "\n",
        "            y1 = max(0, y - self.hole_size // 2)\n",
        "            y2 = min(h, y + self.hole_size // 2)\n",
        "            x1 = max(0, x - self.hole_size // 2)\n",
        "            x2 = min(w, x + self.hole_size // 2)\n",
        "\n",
        "            mask[y1:y2, x1:x2] = 0\n",
        "\n",
        "        img = img * mask.unsqueeze(0)\n",
        "        return img\n",
        "\n",
        "# Defining transformations so they fit all models\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    Cutout(num_holes=1, hole_size=32),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Loads the dataset and category names\n",
        "dataset = torchvision.datasets.ImageFolder(dataset_path, transform=train_transform)\n",
        "class_names = dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Splits the dataset into training, validation, and test sets: 80%, 10% and 10% respectively\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# C1: Transforms for validation and test sets\n",
        "val_dataset.dataset.transform = val_transform\n",
        "test_dataset.dataset.transform = val_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #Shuffles training data to increase robustness/generalization\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) #Shuffling = false to maintain consistent evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) #Shuffling = false to maintain consistent testing of performance\n",
        "\n",
        "# B: Defines models, the pre-trained weights used are behind the following hashtag\n",
        "models_dict = {\n",
        "    \"ResNet50\": models.resnet50(weights=None), #models.ResNet50_Weights.IMAGENET1K_V2\n",
        "    \"MobileNetV3\": models.mobilenet_v3_large(weights=None), #models.MobileNet_V3_Large_Weights.IMAGENET1K_V1\n",
        "    \"EfficientNetV2_S\": models.efficientnet_v2_s(weights=None) #True\n",
        "}\n",
        "\n",
        "# Modifies the final fully connected layer to the correct number of classes\n",
        "for model_name, model in models_dict.items():\n",
        "    if model_name == \"EfficientNetV2_S\":\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    elif model_name == \"MobileNetV3\":\n",
        "        num_ftrs = model.classifier[3].in_features\n",
        "        model.classifier[3] = nn.Linear(num_ftrs, num_classes)\n",
        "    else:\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# C: Implementing Early Stopping and saving the best epoch\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.01, checkpoint_path=\"checkpoint.pth\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), self.checkpoint_path)\n",
        "            print(f\"Model improved, saved to {self.checkpoint_path}\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Function to add Gaussian noise used in testing\n",
        "def add_gaussian_noise(inputs, mean=0.0, std=0.1):\n",
        "    noise = torch.randn_like(inputs) * std + mean\n",
        "    return inputs + noise\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Uses GPU if available, as GPU is much faster for training deep learning models\n",
        "\n",
        "# Storage for training, validation, and tests\n",
        "train_losses = {model_name: [] for model_name in models_dict}\n",
        "val_losses = {model_name: [] for model_name in models_dict}\n",
        "val_accuracies = {model_name: [] for model_name in models_dict}\n",
        "test_accuracies_per_epoch = {model_name: [] for model_name in models_dict}\n",
        "\n",
        "# Training, validation and test loop\n",
        "for model_name, model in models_dict.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=3, min_delta=0.01, checkpoint_path=f\"{model_name}_checkpoint.pth\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Training {model_name} (Epoch {epoch+1}/{num_epochs})\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_losses[model_name].append(avg_train_loss)\n",
        "\n",
        "        # Stores and evaluates the validation phase\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(val_loader)\n",
        "        val_losses[model_name].append(avg_val_loss)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "        val_accuracies[model_name].append(val_accuracy)\n",
        "\n",
        "        # Engages the test phase and prints results\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        test_accuracies_per_epoch[model_name].append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "        early_stopping(avg_val_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping triggered for {model_name}\")\n",
        "            break\n",
        "\n",
        "# Plot training and validation loss for each model\n",
        "for model_name in models_dict.keys():\n",
        "    epochs = range(1, len(train_losses[model_name]) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses[model_name], label=\"Training Loss\", color='blue')\n",
        "    plt.plot(epochs, val_losses[model_name], label=\"Validation Loss\", color='orange')\n",
        "    plt.title(f\"Training and Validation Loss for {model_name}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot test accuracy for all models\n",
        "plt.figure(figsize=(10, 6))\n",
        "for model_name, accuracies in test_accuracies_per_epoch.items():\n",
        "    plt.plot(range(1, len(accuracies) + 1), accuracies, label=model_name)\n",
        "plt.title(\"Test Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot example images with predictions from all models\n",
        "num_images_to_plot = 5\n",
        "sample_inputs, sample_labels = [], []\n",
        "predictions_per_model = {model_name: [] for model_name in models_dict.keys()}\n",
        "\n",
        "# Collect predictions\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        for model_name, model in models_dict.items():\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            predictions_per_model[model_name].extend(predictions.cpu().numpy())\n",
        "\n",
        "        sample_inputs.extend(inputs.cpu())\n",
        "        sample_labels.extend(labels.cpu())\n",
        "\n",
        "        if len(sample_inputs) >= num_images_to_plot:\n",
        "            break\n",
        "\n",
        "# Transforms the images back to normal while keeping noise, and plots them in a 3x3 grid\n",
        "num_images_to_plot = 9\n",
        "sample_inputs, sample_labels = [], []\n",
        "predictions_per_model = {model_name: [] for model_name in models_dict.keys()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        for model_name, model in models_dict.items():\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            predictions_per_model[model_name].extend(predictions.cpu().numpy())\n",
        "\n",
        "        sample_inputs.extend(inputs.cpu())\n",
        "        sample_labels.extend(labels.cpu())\n",
        "\n",
        "        if len(sample_inputs) >= num_images_to_plot:\n",
        "            break\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "for idx in range(num_images_to_plot):\n",
        "    row, col = divmod(idx, 3)\n",
        "    image = sample_inputs[idx].permute(1, 2, 0).numpy()\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    image = std * image + mean\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    # Add back the noise using the earlier noise function\n",
        "    noisy_image = add_gaussian_noise(torch.tensor(image).permute(2, 0, 1)).permute(1, 2, 0).numpy()\n",
        "    noisy_image = noisy_image.clip(0, 1)\n",
        "\n",
        "    # Plot the noisy image with label and the models prediction\n",
        "    axes[row, col].imshow(noisy_image)\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "    predictions_text = \"\\n\".join([f\"{model_name} pred: {class_names[predictions_per_model[model_name][idx]]}\"\n",
        "                                  for model_name in models_dict.keys()])\n",
        "    axes[row, col].set_title(f\"True: {class_names[sample_labels[idx].item()]}\\n{predictions_text}\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}